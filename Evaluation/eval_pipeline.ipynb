{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install langchain-openai\n",
    "!pip install pysbd\n",
    "#!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "\n",
    "import gc\n",
    "\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader, PyPDFLoader, TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.evaluation import Criteria\n",
    "from langchain.evaluation import load_evaluator, EvaluatorType\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clear_gpu_memory():\n",
    "    gc.collect()\n",
    "    cuda.empty_cache()\n",
    "    print(\"GPU memory has been freed.\")\n",
    "\n",
    "\n",
    "def report_memory_usage(cuda_device_id):\n",
    "    # Set PyTorch memory allocation configuration\n",
    "    torch.backends.cuda.max_split_size_mb = 0\n",
    "\n",
    "    # Example usage:\n",
    "    # Place this function call at appropriate points in your code to free up GPU memory\n",
    "    # clear_gpu_memory()\n",
    "\n",
    "    # Check available GPU memory\n",
    "    gpu_memory_info = torch.cuda.memory_stats()\n",
    "\n",
    "    # Print all keys in gpu_memory_info\n",
    "    print(\"Keys in gpu_memory_info:\", gpu_memory_info.keys())\n",
    "\n",
    "    # Extract and print specific information\n",
    "    free_memory = gpu_memory_info.get('free_bytes', 0) / 1024**2  # Convert to MiB\n",
    "    used_memory = gpu_memory_info.get('allocated_bytes.all.peak', 0) / 1024**2  # Convert to MiB\n",
    "    process_memory = gpu_memory_info.get('allocated_bytes.current', 0) / 1024**2  # Convert to MiB\n",
    "\n",
    "    print(f\"{free_memory:.2f} MiB is free.\")\n",
    "    print(f\"Including non-PyTorch memory, this process has {used_memory:.2f} GiB memory in use.\")\n",
    "    print(f\"Process on cuda device {cuda_device_id} has {process_memory:.2f} GiB memory in use.\")\n",
    "\n",
    "    # Check PyTorch memory allocation details\n",
    "    pytorch_memory_info = torch.cuda.memory_reserved()\n",
    "\n",
    "    print(f\"Total GPU memory reserved by PyTorch: {pytorch_memory_info / 1024**2:.2f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_model(model_id, cuda_device='cuda:0', max_length=512): \n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=bfloat16\n",
    "    )\n",
    "\n",
    "    model_config = transformers.AutoConfig.from_pretrained(\n",
    "        model_id\n",
    "    )\n",
    "    \n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        trust_remote_code=True,\n",
    "        config=model_config,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=cuda_device,\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def prepare_pipeline(model, tokenizer, cuda_device='cuda:1', max_new_tokens=512, temperature=1e-4, repetition_penalty=1.1):\n",
    "    query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=cuda_device,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        return_full_text=True,\n",
    "    )\n",
    "    \n",
    "    return HuggingFacePipeline(pipeline=query_pipeline)\n",
    "\n",
    "def get_answers(qa, retriever, var_values):\n",
    "    questions = [\"What did the president say about Justice Breyer?\", \n",
    "             \"What did the president say about Intel's CEO?\",\n",
    "             \"What did the president say about gun violence?\",\n",
    "            ]\n",
    "\n",
    "    ground_truth = [\"The president said that Justice Breyer has dedicated his life to serve the country and thanked him for his service.\",\n",
    "                    \"The president said that Pat Gelsinger is ready to increase Intel's investment to $100 billion.\",\n",
    "                    \"The president asked Congress to pass proven measures to reduce gun violence.\"]\n",
    "    answers = []\n",
    "    contexts = []\n",
    "    sources = []\n",
    "    model_ids = []\n",
    "    emb_function = []\n",
    "    chunk_size = []\n",
    "    chunk_overlap = []\n",
    "    textsplitter = []\n",
    "    vectordb = []\n",
    "    retriever_k = []\n",
    "    temp_prompt = []\n",
    "\n",
    "    for question in questions:\n",
    "        cont = []\n",
    "        src = []\n",
    "        answers.append(qa.invoke(question)[\"result\"])\n",
    "        for context in retriever.get_relevant_documents(question):\n",
    "            cont.append(context.page_content)\n",
    "            src.append(context.metadata[\"source\"])\n",
    "        # contexts.append([context.page_content for context in retriever.get_relevant_documents(question)])\n",
    "        contexts.append(cont)\n",
    "        sources.append(src)\n",
    "\n",
    "        model_ids.append(var_values[\"model_id\"])\n",
    "        emb_function.append(var_values[\"emb_function\"])\n",
    "        chunk_size.append(var_values[\"chunk_size\"])\n",
    "        chunk_overlap.append(var_values[\"chunk_overlap\"])\n",
    "        textsplitter.append(var_values[\"textsplitter\"])\n",
    "        vectordb.append(var_values[\"vectordb\"])\n",
    "        retriever_k.append(var_values[\"retriever_k\"])\n",
    "        temp_prompt.append(var_values[\"temp_prompt\"])\n",
    "    \n",
    "\n",
    "    data = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": ground_truth,\n",
    "        \"sources\": sources,\n",
    "        \"model_id\": model_ids,\n",
    "        \"emb_function\": emb_function,\n",
    "        \"chunk_size\": chunk_size,\n",
    "        \"chunk_overlap\": chunk_overlap,\n",
    "        \"textsplitter\": textsplitter,\n",
    "        \"vectordb\": vectordb,\n",
    "        \"retriever_k\": retriever_k,\n",
    "        \"temp_prompt\": temp_prompt\n",
    "    }\n",
    "\n",
    "    dataset = Dataset.from_dict(data)\n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "    return df, dataset\n",
    "\n",
    "\n",
    "# merge_axis: 0: merge rows, 1: merge columns\n",
    "def merge_datasets(dataset_list, merge_axis=0):\n",
    "    if len(dataset_list) > 0:\n",
    "        pd_dfs = [dataset.to_pandas() for dataset in dataset_list]\n",
    "\n",
    "        for i,df in enumerate(pd_dfs):\n",
    "            if i==0:\n",
    "                merged_df = df\n",
    "            else:\n",
    "                merged_df = pd.concat([merged_df, df], axis=merge_axis)\n",
    "\n",
    "        merged_df.reset_index(drop=True, inplace=True)\n",
    "        return merged_df, Dataset.from_pandas(merged_df)\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "\n",
    "def save_df(df,dir,file_name):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    df.to_csv(os.path.join(dir,file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_dir = \"results\"\n",
    "save_file_name = \"results_multi_query.csv\"\n",
    "save_file_name_eval = \"eval_results_multi_query.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cuda_device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "eval_device = cuda_device\n",
    "\n",
    "eval_model_id = \"mistralai/Mixtral-8x7B\"\n",
    "\n",
    "text_loader_kwargs = {'autodetect_encoding': True}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c51be80ca249e695438ae6fa2afde1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 08:09:06.591467: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:519: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  if not is_torch_available():\n"
     ]
    }
   ],
   "source": [
    "eval_model, eval_tokenizer = prepare_model(eval_model_id,eval_device)\n",
    "eval_llm = prepare_pipeline(eval_model, eval_tokenizer, eval_device, max_new_tokens=4096)\n",
    "\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=\"BAAI/bge-large-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset from file (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "semantic_chunking_both_gens = None\n",
    "with open(\"multi_query_test_sk8x7B\", 'rb') as fp:\n",
    "    semantic_chunking_both_gens = pickle.load(fp)\n",
    "test_log = semantic_chunking_both_gens\n",
    "eval_list = [\n",
    "    {\n",
    "        \"question\": log[\"query\"],\n",
    "        \"contexts\": [doc.page_content for doc, _ in log[\"context\"]],\n",
    "        \"ground_truth\": log[\"kwargs\"][\"ground_truths\"],\n",
    "        \"answer\": log[\"inference\"],\n",
    "        \"chunk_strategie\": log[\"kwargs\"][\"chunk_strategie\"],\n",
    "        \"multi_query\": log[\"multi_query\"],\n",
    "        \"documents_in_scope\": log[\"documents_in_scope\"],\n",
    "        \"retrieve_methods\": log[\"retrieve_methods\"]\n",
    "    }\n",
    "    for log in test_log#.to_dict(\"records\")\n",
    "]\n",
    "dataset = pd.DataFrame(data=eval_list)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.to_excel(\"multi_query_test_sk8x7B.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'contexts', 'ground_truth', 'answer', 'chunk_strategie', 'retrieve_methods'],\n",
       "    num_rows: 328\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"contexts\"] = dataset.apply(lambda x: x[\"contexts\"] if len(x[\"contexts\"]) > 0 else np.nan, axis = 1)\n",
    "dataset = dataset.dropna()\n",
    "dataset.reset_index(inplace=True, drop=True)\n",
    "merged_df = dataset\n",
    "merged_dataset = Dataset.from_pandas(dataset)\n",
    "merged_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragas metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "handler = StdOutCallbackHandler()\n",
    "ragas_result_dataset = evaluate(\n",
    "     dataset = merged_dataset, \n",
    "     metrics=[\n",
    "         context_precision,\n",
    "         context_recall,\n",
    "         #faithfulness,\n",
    "         #answer_relevancy,\n",
    "     ],\n",
    "    #callbacks = [handler],\n",
    "    llm=eval_llm,\n",
    "    embeddings=embedding_function\n",
    ")\n",
    "\n",
    "ragas_result_df = ragas_result_dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ragas_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain metrics\n",
    "[<Criteria.CONCISENESS: 'conciseness'>,\n",
    " <Criteria.RELEVANCE: 'relevance'>,\n",
    " <Criteria.CORRECTNESS: 'correctness'>,\n",
    " <Criteria.COHERENCE: 'coherence'>,\n",
    " <Criteria.HARMFULNESS: 'harmfulness'>,\n",
    " <Criteria.MALICIOUSNESS: 'maliciousness'>,\n",
    " <Criteria.HELPFULNESS: 'helpfulness'>,\n",
    " <Criteria.CONTROVERSIALITY: 'controversiality'>,\n",
    " <Criteria.MISOGYNY: 'misogyny'>,\n",
    " <Criteria.CRIMINALITY: 'criminality'>,\n",
    " <Criteria.INSENSITIVITY: 'insensitivity'>,\n",
    " <Criteria.DEPTH: 'depth'>,\n",
    " <Criteria.CREATIVITY: 'creativity'>,\n",
    " <Criteria.DETAIL: 'detail'>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from langchain.evaluation.criteria.eval_chain import (\n",
    "    CriteriaEvalChain,\n",
    "    LabeledCriteriaEvalChain,\n",
    ")\n",
    "langchain_eval_metrics = [\n",
    "    \"conciseness\", # Is the submission concise and to the point?\n",
    "    \"relevance\", # Is the submission referring to a real quote from the text?\n",
    "    \"correctness\", # Is the submission correct, accurate, and factual?\n",
    "    \"helpfulness\", # Is the submission helpful, insightful, and appropriate?\n",
    "    \"detail\" # Does the submission demonstrate attention to detail?   \n",
    "]\n",
    "\n",
    "# criterias = [crit.value for crit in list(Criteria)]\n",
    "eval_results = []\n",
    "\n",
    "for eval_metric in tqdm(langchain_eval_metrics):\n",
    "    if eval_metric == \"correctness\":\n",
    "        evaluator = load_evaluator(\"labeled_criteria\", llm=eval_llm, criteria=eval_metric)\n",
    "    else:\n",
    "        evaluator = load_evaluator(EvaluatorType.LABELED_CRITERIA,\n",
    "                                   #\"criteria\",\n",
    "                                   llm=eval_llm, criteria=eval_metric)\n",
    "    \n",
    "    eval_metric_results = []\n",
    "\n",
    "    for index, row in merged_df.iterrows():\n",
    "        eval_result = evaluator.evaluate_strings(\n",
    "        input = row[\"question\"],\n",
    "        prediction = row[\"answer\"],\n",
    "        reference = row[\"ground_truth\"])\n",
    "\n",
    "        eval_metric_results.append(eval_result[\"score\"])\n",
    "\n",
    "    eval_results.append(eval_metric_results)\n",
    "\n",
    "print(eval_results)    \n",
    "langchain_eval_df = pd.DataFrame(list(zip(*eval_results)), columns=langchain_eval_metrics)\n",
    "langchain_eval_dataset = Dataset.from_pandas(langchain_eval_df)\n",
    "\n",
    "all_eval_results_df, all_eval_results_dataset = merge_datasets([ragas_result_dataset,langchain_eval_dataset],1)\n",
    "#save_df(all_eval_results_df,save_dir,save_file_name_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_eval_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(\"LC_Crit_Llama-2-13b\", 'wb') as fp:\n",
    "          pickle.dump(all_eval_results_df, fp)\n",
    "          print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
