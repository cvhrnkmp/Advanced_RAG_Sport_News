{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68e484b4-3b48-41ac-8241-133fa6251e50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ragas in /opt/conda/lib/python3.10/site-packages (0.1.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ragas) (1.23.5)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from ragas) (2.16.1)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from ragas) (0.5.2)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (from ragas) (0.1.12)\n",
      "Requirement already satisfied: langchain-core in /opt/conda/lib/python3.10/site-packages (from ragas) (0.1.32)\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.10/site-packages (from ragas) (0.0.28)\n",
      "Requirement already satisfied: langchain-openai in /opt/conda/lib/python3.10/site-packages (from ragas) (0.0.8)\n",
      "Requirement already satisfied: openai>1 in /opt/conda/lib/python3.10/site-packages (from ragas) (1.14.1)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in /opt/conda/lib/python3.10/site-packages (from ragas) (0.3.4)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ragas) (1.5.6)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (3.6.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (0.26.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (4.9.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (3.9.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (12.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (2.29.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (2023.5.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (0.20.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (2.0.13)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (1.33)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (0.1.27)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (8.2.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->ragas) (2023.12.25)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragas) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragas) (0.9.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2023.5.7)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain->ragas) (2.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai>1->ragas) (2.14.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->ragas) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets->ragas) (1.26.15)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->ragas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->ragas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->ragas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->ragas) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcdb2546-90b4-4957-8149-2245ba98ee91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")\n",
    "\n",
    "from ragas import evaluate\n",
    "import torch\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.text_splitter import TokenTextSplitter, Tokenizer, TextSplitter\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a7bc9d13-7b77-4eaf-9f6c-f96ee23c7fc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBD_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#EMBD_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#GEN_MODEL = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "GEN_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "AUTH_TOKEN = \"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\"\n",
    "DEVICE = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8881be26-63eb-4d9e-972a-5883bebd4786",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomHuggingfaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id, api_key, **model_kwargs):\n",
    "        super().__init__()\n",
    "        self.model_name = model_id\n",
    "        self.api_key = api_key\n",
    "        if isinstance(model_kwargs, dict):\n",
    "            self.model_config = model_kwargs\n",
    "        \n",
    "        self.InitializeModel()\n",
    "        \n",
    "        \n",
    "    def InitializeModel(self):\n",
    "        # Load model from HuggingFace Hub\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name,\n",
    "                                              config = self.model_config)\n",
    "        self.model.to(DEVICE)\n",
    "        print(\"Model on CUDA: \", str(next(self.model.parameters()).is_cuda))\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "    def embed_query(self, text:str, mean_pooling: bool = True) -> List[float]:\n",
    "        return self.embed_documents([text], mean_pooling)[0]\n",
    "    \n",
    "    def embed_documents(self, texts: List[str], mean_pooling: bool = True) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Pitfalls: It's important to pass texts, that fits in the max_sequenz_length of the model to avoid index error.\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"Tokenizer on CUDA: \", str(next(self.tokenizer.parameters()).is_cuda))\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                #print(\"Text: \", text)\n",
    "                text_tokens = self.tokenizer(text, return_tensors='pt', add_special_tokens=False).to(DEVICE)\n",
    "                #print(\"text_tokens: \", len(text_tokens[\"input_ids\"][0]))\n",
    "                #print(text)\n",
    "                #print(\"Len of attention mask: \", str(len(text_tokens[\"attention_mask\"][0])))\n",
    "                #print()\n",
    "                embedding = self.model(**text_tokens)\n",
    "\n",
    "                if mean_pooling:\n",
    "                    embedding = self.mean_pooling(embedding.copy(), text_tokens['attention_mask'])\n",
    "\n",
    "                embeddings.append(embedding[0].cpu().tolist())\n",
    "            #print(embeddings[:2])\n",
    "        return embeddings\n",
    "\n",
    "    def split_text_on_tokens(self, *, text: str, tokenizer: Tokenizer) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\n",
    "        splits: List[str] = []\n",
    "        input_ids = tokenizer.encode(text)\n",
    "        start_idx = 0\n",
    "        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "        chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        while start_idx < len(input_ids):\n",
    "            splits.append(tokenizer.decode(chunk_ids))\n",
    "            if cur_idx == len(input_ids):\n",
    "                break\n",
    "            start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n",
    "            cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "            chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3180921-2925-4bfb-ad67-f1076b953aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37392ee74ddd4e67a86ccc0cc4c675cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384bf7aa0ff347b9b5f6f25692b39ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = GEN_MODEL\n",
    "auth_token = AUTH_TOKEN\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    output_hidden_states=True,\n",
    "    use_auth_token=auth_token,\n",
    ")\n",
    "\n",
    "\n",
    "# Load model\n",
    "auth_token = \"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\"  # The authorization code is insible to the public\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=auth_token, device = \"auto\")\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            trust_remote_code=True,\n",
    "                                            config=model_config,\n",
    "                                            quantization_config=bnb_config,\n",
    "                                            device_map=\"auto\",\n",
    "                                            use_auth_token=auth_token\n",
    "                                            )\n",
    "\n",
    "generation_pipeline = transformers.pipeline(\n",
    "    model=gpt_model,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    temperature=1e-8,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06de64d8-fb9e-4224-82a1-d7d4666a3578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on CUDA:  True\n"
     ]
    }
   ],
   "source": [
    "embeddings_model = CustomHuggingfaceEmbeddings(model_id = EMBD_MODEL, \n",
    "                                           api_key = AUTH_TOKEN, \n",
    "                                           model_kwargs = {\"device\": DEVICE})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0edc3f84-e8ad-47ee-9d2e-b45c8d3b4715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "generator_llm = LangchainLLMWrapper(llm)\n",
    "embeddings_ragas = LangchainEmbeddingsWrapper(embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c7f106a-8568-4f60-946f-5ec7581368ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/datasets/load.py:1429: FutureWarning: The repository for explodinggradients/amnesty_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/explodinggradients/amnesty_qa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# loading the V2 dataset\n",
    "amnesty_qa = load_dataset(\"explodinggradients/amnesty_qa\", \"english_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d4aedc70-7c5f-4e76-bcf8-42e3a9c510d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "test_log = None\n",
    "with open(\"test_log\", 'rb') as fp:\n",
    "    test_log = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7eeb9fb7-3252-477f-9c67-7193a598d6cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wie hat Leverkusen am 2023-11-25 in der 1.Bundesliga gespielt?'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_log[0][\"query\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2baed485-99bf-4e6d-9590-14a1e6345aec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da7a96f8-ca72-41d8-bf1b-74572f58a1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_list = [\n",
    "    {\n",
    "        \"question\": log[\"query\"],\n",
    "        \"contexts\": [doc.page_content for doc, _ in log[\"context\"]],\n",
    "        \"answer\": log[\"inference\"],\n",
    "        \n",
    "    }\n",
    "    for log in test_log\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c353361a-501f-405d-99fb-8cca6168e9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'contexts', 'answer'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.Dataset.from_pandas(pd.DataFrame(data=eval_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b514fcc2-12cd-4496-99e3-928481d10ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wie hat Leverkusen am 2023-11-25 in der 1.Bund...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Thanks for asking! Leverkusen has played agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wie hat Leverkusen am 2023-11-25 in der 1.Bund...</td>\n",
       "      <td>[leverkusen hat die tabellenfuhrung in der fuß...</td>\n",
       "      <td>Leverkusen hat am 2023-11-25 in der 1.Bundesl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wie hat Leverkusen am 2023-11-25 in der 1.Bund...</td>\n",
       "      <td>[und weil niklas sule wegen einer grippe fehlt...</td>\n",
       "      <td>Leverkusen hat am 2023-11-25 in der 1.Bundesl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wie hat Leverkusen am 2023-11-25 in der 1.Bund...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Thanks for asking! Leverkusen has played agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wie hat Leverkusen am 2023-11-25 in der 1.Bund...</td>\n",
       "      <td>[leverkusen hat die tabellenfuhrung in der fuß...</td>\n",
       "      <td>Leverkusen hat am 2023-11-25 in der 1.Bundesl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Wie hat Leverkusen am 2023-11-25 in der 1.Bund...</td>\n",
       "      <td>[leverkusen hat die tabellenfuhrung in der fuß...</td>\n",
       "      <td>Leverkusen hat am 2023-11-25 in der 1.Bundesl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wie hat Leverkusen am 2023-11-25 in der 1.Bund...</td>\n",
       "      <td>[und weil niklas sule wegen einer grippe fehlt...</td>\n",
       "      <td>Leverkusen hat am 2023-11-25 in der 1.Bundesl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Wie hat Leverkusen am 2023-11-25 in der 1.Bund...</td>\n",
       "      <td>[leverkusen hat die tabellenfuhrung in der fuß...</td>\n",
       "      <td>Leverkusen hat am 2023-11-25 in der 1.Bundesl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question   \n",
       "0  Wie hat Leverkusen am 2023-11-25 in der 1.Bund...  \\\n",
       "1  Wie hat Leverkusen am 2023-11-25 in der 1.Bund...   \n",
       "2  Wie hat Leverkusen am 2023-11-25 in der 1.Bund...   \n",
       "3  Wie hat Leverkusen am 2023-11-25 in der 1.Bund...   \n",
       "4  Wie hat Leverkusen am 2023-11-25 in der 1.Bund...   \n",
       "5  Wie hat Leverkusen am 2023-11-25 in der 1.Bund...   \n",
       "6  Wie hat Leverkusen am 2023-11-25 in der 1.Bund...   \n",
       "7  Wie hat Leverkusen am 2023-11-25 in der 1.Bund...   \n",
       "\n",
       "                                            contexts   \n",
       "0                                                 []  \\\n",
       "1  [leverkusen hat die tabellenfuhrung in der fuß...   \n",
       "2  [und weil niklas sule wegen einer grippe fehlt...   \n",
       "3                                                 []   \n",
       "4  [leverkusen hat die tabellenfuhrung in der fuß...   \n",
       "5  [leverkusen hat die tabellenfuhrung in der fuß...   \n",
       "6  [und weil niklas sule wegen einer grippe fehlt...   \n",
       "7  [leverkusen hat die tabellenfuhrung in der fuß...   \n",
       "\n",
       "                                              answer  \n",
       "0   Thanks for asking! Leverkusen has played agai...  \n",
       "1   Leverkusen hat am 2023-11-25 in der 1.Bundesl...  \n",
       "2   Leverkusen hat am 2023-11-25 in der 1.Bundesl...  \n",
       "3   Thanks for asking! Leverkusen has played agai...  \n",
       "4   Leverkusen hat am 2023-11-25 in der 1.Bundesl...  \n",
       "5   Leverkusen hat am 2023-11-25 in der 1.Bundesl...  \n",
       "6   Leverkusen hat am 2023-11-25 in der 1.Bundesl...  \n",
       "7   Leverkusen hat am 2023-11-25 in der 1.Bundesl...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame(data=eval_list)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc441959-0734-43cd-a60f-ec9833478f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[\"contexts\"] = dataset.apply(lambda x: x[\"contexts\"] if len(x[\"contexts\"]) > 0 else np.nan, axis = 1)\n",
    "dataset = dataset.dropna()\n",
    "dataset.reset_index(inplace=True, drop=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a5d3f30-cb71-4fda-b504-741e181ea9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ev = datasets.Dataset.from_pandas(dataset[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b9ec6-acb5-4d3a-9894-6e2f53a9dd35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "616e736f-9d0f-417e-88be-7df05ee68412",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55609a55f4bf43a784235c236895b526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Invalid JSON response. Expected dictionary with key 'question'\n",
      "/opt/conda/lib/python3.10/site-packages/ragas/evaluation.py:276: RuntimeWarning: Mean of empty slice\n",
      "  value = np.nanmean(self.scores[cn])\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(\n",
    "                    ev, \n",
    "                    metrics=[answer_relevancy, faithfulness], \n",
    "                    llm=generator_llm, \n",
    "                    embeddings=embeddings_ragas\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "effa4cea-73d6-4d84-9485-f74ae91c2bf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>faithfulness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wie hat Leverkusen am 2023-11-25 in der 1.Bund...</td>\n",
       "      <td>[leverkusen hat die tabellenfuhrung in der fuß...</td>\n",
       "      <td>Leverkusen hat am 2023-11-25 in der 1.Bundesl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question   \n",
       "0  Wie hat Leverkusen am 2023-11-25 in der 1.Bund...  \\\n",
       "\n",
       "                                            contexts   \n",
       "0  [leverkusen hat die tabellenfuhrung in der fuß...  \\\n",
       "\n",
       "                                              answer  answer_relevancy   \n",
       "0   Leverkusen hat am 2023-11-25 in der 1.Bundesl...               NaN  \\\n",
       "\n",
       "   faithfulness  \n",
       "0           NaN  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = results.to_pandas()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3042220d-5d3a-4ab9-afb1-873a61315b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0715db21-c089-4725-af35-5df1ee0cd421",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
