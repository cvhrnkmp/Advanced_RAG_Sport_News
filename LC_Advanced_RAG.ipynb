{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb1f2b-e730-4ded-844c-8f6f0dfeaaee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install gliner\n",
    "!pip install sentence_transformers\n",
    "!pip install huggingface_hub\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "#!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install transformers==4.38.2\n",
    "!pip install jq\n",
    "!pip install dateparser\n",
    "!pip install chromadb\n",
    "!pip install rank_bm25\n",
    "!pip install langchain_community\n",
    "!pip install langchain\n",
    "!pip install langchain_experimental\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0d51a0-3c21-4a6f-952d-72e42fc4b2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/chvah002/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "#from gliner import GLiNER\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from transformers import BitsAndBytesConfig\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.runnables import RunnableParallel\n",
    "from langchain.load import dumps, loads\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import langchain\n",
    "import re\n",
    "from sentence_transformers import util\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import spacy\n",
    "from spacy.lang.de.examples import sentences \n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "import json\n",
    "import pickle\n",
    "from langchain_community.document_transformers import (\n",
    "    LongContextReorder,\n",
    ")\n",
    "\n",
    "langchain.debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fa9e9b8-55a4-48d8-b5aa-fcc712d8981c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBD_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "#EMBD_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#GEN_MODEL = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "GEN_MODEL = \"VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct\"\n",
    "#GEN_MODEL = \"VAGOsolutions/SauerkrautLM-SOLAR-Instruct\"\n",
    "AUTH_TOKEN = \"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\"\n",
    "USE_RETRIEVER = [\n",
    "    \"similarity\", \n",
    "    \"time_weigthed\", \n",
    "    \"keyword\"\n",
    "                ]\n",
    "DEVICE = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b9a789-e7d1-4852-a811-ad731b435882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read data\n",
    "file_path='../revier_output.json'\n",
    "data = pd.read_json(file_path)\n",
    "\n",
    "loader = DataFrameLoader(data, page_content_column=\"content\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Document mapping\n",
    "def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "    metadata[\"source\"] = record.get(\"url\")\n",
    "    metadata[\"description\"] = record.get(\"description\")\n",
    "    metadata[\"news_keywords\"] = record.get(\"news_keywords\")\n",
    "    #metadata[\"published_date\"] = datetime.strptime(record.get(\"published_date\").strip(),\"%Y-%m-%d %H:%M:%S\") # 2018-12-18 18:52:00\n",
    "    metadata[\"published_date\"] = record.get(\"published_date\")\n",
    "    return metadata\n",
    "\n",
    "# Init JSONLoader\n",
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.[]',\n",
    "    content_key= \"content\",\n",
    "    metadata_func=metadata_func,\n",
    "\n",
    ")\n",
    "\n",
    "# Load data\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443cc36b-624a-48bc-bb6b-310549c69b44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:720: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:466: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "370d3935cce74431b3ad17357920dbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-15 07:24:08.211000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# -- Create LLM Pipeline to pass it as ready-to-use to RAG --\n",
    "\n",
    "# Define model and auth_token\n",
    "model_id = GEN_MODEL\n",
    "auth_token = AUTH_TOKEN\n",
    "\n",
    "# Quantization settings\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "# Model configs\n",
    "model_config = transformers.AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    output_hidden_states=True,\n",
    "    use_auth_token=auth_token,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "auth_token = \"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\"  # The authorization code is insible to the public\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=auth_token, device = \"auto\")\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                            trust_remote_code=True,\n",
    "                                            config=model_config,\n",
    "                                            quantization_config=bnb_config,\n",
    "                                            device_map=\"auto\",\n",
    "                                            use_auth_token=auth_token\n",
    "                                            )\n",
    "# Create generation pipeline\n",
    "generation_pipeline = transformers.pipeline(\n",
    "    model=gpt_model,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1,  # without this output begins repeating\n",
    "    do_sample = True\n",
    ")\n",
    "# Init Langchain Pipeline\n",
    "llm = HuggingFacePipeline(pipeline=generation_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6869df1f-9158-49e3-b97d-16ee0c2fa168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.embeddings import Embeddings\n",
    "from typing import List\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from langchain.text_splitter import TokenTextSplitter, Tokenizer, TextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn.functional as F\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "from typing import Any, Callable, Optional, cast, Iterable, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "# from langchain_community.vectorstores import FAISS\n",
    "# import faiss\n",
    "from langchain_core.documents import Document\n",
    "from GLiNER.gliner_ner import GlinerNER\n",
    "from dateparser.search import search_dates\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "from typing import List, Optional\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time\n",
    "from langchain_community.utils.math import (\n",
    "    cosine_similarity,\n",
    ")\n",
    "\n",
    "class CustomHuggingfaceEmbeddings(Embeddings):\n",
    "    def __init__(self, model_id, api_key, **model_kwargs):\n",
    "        super().__init__()\n",
    "        self.model_name = model_id\n",
    "        self.api_key = api_key\n",
    "        if isinstance(model_kwargs, dict):\n",
    "            self.model_config = model_kwargs\n",
    "        \n",
    "        self.InitializeModel()\n",
    "        \n",
    "    def InitializeModel(self):\n",
    "        # Load model from HuggingFace Hub\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModel.from_pretrained(self.model_name,\n",
    "                                              config = self.model_config)\n",
    "        self.model.to(DEVICE)\n",
    "        print(\"Model on CUDA: \", str(next(self.model.parameters()).is_cuda))\n",
    "    #Mean Pooling - Take attention mask into account for correct averaging\n",
    "    def mean_pooling(self, model_output, attention_mask):\n",
    "        token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "        \n",
    "    def embed_query(self, text:str, mean_pooling: bool = True) -> List[float]:\n",
    "        return self.embed_documents([text], mean_pooling)[0]\n",
    "    \n",
    "    def embed_documents(self, texts: List[str], mean_pooling: bool = True) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Pitfalls: It's important to pass texts, that fits in the max_sequenz_length of the model to avoid index error.\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        with torch.no_grad():\n",
    "            for text in texts:\n",
    "                text_tokens = self.tokenizer(text, return_tensors='pt', add_special_tokens=False).to(DEVICE)\n",
    "                if len(text_tokens[\"input_ids\"][0]) <= 512 and len(text_tokens[\"input_ids\"][0]) > 0:\n",
    "                    embedding = self.model(**text_tokens)\n",
    "                    if mean_pooling:\n",
    "                        embedding = self.mean_pooling(embedding.copy(), text_tokens['attention_mask'])\n",
    "                    embd = embedding[0].cpu().tolist()\n",
    "                    if len(embd) > 0:\n",
    "                        self.counter += 1\n",
    "                        embeddings.append(embd)\n",
    "                    else:\n",
    "                        embeddings.append(None)\n",
    "                else:\n",
    "                    print(text)\n",
    "                    print(\"len of text:\", str(len(text_tokens[\"input_ids\"][0])))\n",
    "                    print(len(text_tokens[\"input_ids\"][0]))\n",
    "        return embeddings\n",
    "    \n",
    "    \n",
    "    def split_text_on_tokens(self, *, text: str, tokenizer: Tokenizer = None) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\n",
    "        if tokenizer == None:\n",
    "            tokenizer = self.tokenizer\n",
    "        splits: List[str] = []\n",
    "        input_ids = tokenizer._encode(text)\n",
    "        \n",
    "        start_idx = 0\n",
    "        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "        chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        while start_idx < len(input_ids):\n",
    "            splits.append(tokenizer._decode(chunk_ids))\n",
    "            if cur_idx == len(input_ids):\n",
    "                break\n",
    "            start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n",
    "            cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "            chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        return splits\n",
    "\n",
    "class CustomTimeWeightedVectorStoreRetriever(TimeWeightedVectorStoreRetriever):\n",
    "    gliner_model: Optional[GlinerNER] = None\n",
    "    def __init__(self, vectorstore, decay_rate = 0.1 , k = 10, **search_kwargs):\n",
    "        super().__init__(\n",
    "            vectorstore = vectorstore,\n",
    "            decay_rate = decay_rate,\n",
    "            search_kwargs = { \"k\": k}\n",
    "        )\n",
    "        self.k = k\n",
    "        self.gliner_model = GlinerNER()\n",
    "    \n",
    "    def date_processing(self, query):\n",
    "        reg_dates = self.gliner_model.predict_tags(query, verbose = True)\n",
    "        print(\"Dates:\\n\", reg_dates)\n",
    "        result = [search_dates(item[\"text\"]) for item in reg_dates if item[\"label\"] in [\"date\"]]\n",
    "        result = [item for item in result if item is not None]\n",
    "        print(result)\n",
    "        return result\n",
    "                          \n",
    "    def _get_relevant_documents(\n",
    "        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Return documents that are relevant to the query.\"\"\"\n",
    "        date_text, date_time = zip(*self.date_processing(query)[0])\n",
    "        date_time = date_time[0]\n",
    "        \n",
    "        documents = self.vectorstore.get(include = [\"metadatas\", \"documents\"])\n",
    "        \n",
    "        docs = [Document(page_content = documents[\"documents\"][idx], metadata=documents[\"metadatas\"][idx]) for idx, doc in enumerate(documents[\"documents\"])]\n",
    "        scored_docs = [(doc, self._get_combined_score(doc, current_time = date_time)) for doc in docs]\n",
    "        scored_docs.sort(key = lambda x: x[1], reverse = True)\n",
    "        result = [doc for doc, score in scored_docs[:self.k]]        \n",
    "        return result\n",
    "    \n",
    "    def _get_combined_score(self,document: Document,current_time: datetime) -> float:\n",
    "        \"\"\"Return the combined score for a document.\"\"\"\n",
    "        hours_passed = self._get_hours_passed(\n",
    "            current_time,\n",
    "            self._document_get_date(\"published_date\", document),\n",
    "        )\n",
    "        score = (1.0 - self.decay_rate) ** hours_passed\n",
    "        return score\n",
    "    \n",
    "    def _get_hours_passed(self, time: datetime, ref_time: datetime) -> float:\n",
    "        \"\"\"Get the hours passed between two datetimes.\"\"\"\n",
    "        date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "        if isinstance(time, str): time = datetime.strptime(time, date_format)\n",
    "        if isinstance(ref_time, str): ref_time = datetime.strptime(ref_time, date_format)\n",
    "        return abs(time - ref_time).total_seconds() / 3600\n",
    "    \n",
    "class CustomMultiQueryRetriever(MultiQueryRetriever):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "    \n",
    "    def generate_queries(\n",
    "        self, question: str, run_manager: CallbackManagerForRetrieverRun\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate queries based upon user input.\n",
    "\n",
    "        Args:\n",
    "            question: user query\n",
    "\n",
    "        Returns:\n",
    "            List of LLM generated queries that are similar to the user input\n",
    "        \"\"\"\n",
    "        response = self.llm_chain(\n",
    "            {\"question\": question}, callbacks=run_manager.get_child()\n",
    "        )\n",
    "        lines = response[\"text\"]\n",
    "        lines = [line.strip() for line in lines if len(line.strip()) > 0]\n",
    "        if self.verbose:\n",
    "            logger.info(f\"Generated queries: {lines}\")\n",
    "        return lines \n",
    "\n",
    "class CostumTokenizer(TextSplitter):\n",
    "    \n",
    "    def __init__(self,\n",
    "                    model_name: str,\n",
    "                    chunk_overlap: int = 50,\n",
    "                    tokens_per_chunk: Optional[int] = None,\n",
    "                    **kwargs: Any) -> None:\n",
    "        # Create a new Textplitter\n",
    "        super().__init__(chunk_overlap =chunk_overlap)\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(EMBD_MODEL)\n",
    "        self._initialize_chunk_configuration(tokens_per_chunk = tokens_per_chunk)\n",
    "    \n",
    "    def _initialize_chunk_configuration(self, *, tokens_per_chunk: Optional[int]) -> None:\n",
    "        self.maximum_tokens_per_chunk = cast(int, self.tokenizer.model_max_length)\n",
    "        \n",
    "        if tokens_per_chunk is None:\n",
    "            self.tokens_per_chunk = self.maximum_tokens_per_chunk\n",
    "        else:\n",
    "            self.tokens_per_chunk = tokens_per_chunk\n",
    "\n",
    "        if self.tokens_per_chunk > self.maximum_tokens_per_chunk:\n",
    "            raise ValueError(\n",
    "                f\"The token limit of the models '{self.model_name}'\"\n",
    "                f\" is: {self.maximum_tokens_per_chunk}.\"\n",
    "                f\" Argument tokens_per_chunk={self.tokens_per_chunk}\"\n",
    "                f\" > maximum token limit.\"\n",
    "            )\n",
    "                                             \n",
    "    def count_tokens(self, *, text: str) -> int:\n",
    "        return len(self._encode(text))\n",
    "    \n",
    "    def _encode(self, text: str) -> List[int]:\n",
    "        token_ids_with_start_and_end_token_ids = self.tokenizer.encode(\n",
    "            text,\n",
    "            add_special_tokens = False,\n",
    "            max_length = self.tokens_per_chunk,\n",
    "            truncation=False,\n",
    "        )\n",
    "        return token_ids_with_start_and_end_token_ids\n",
    "    \n",
    "    def _decode(self, token_ids) -> str:\n",
    "        return self.tokenizer.decode(token_ids)\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:\n",
    "        def encode_strip_start_and_stop_token_ids(text: str) -> List[int]:\n",
    "            return self._encode(text)[1:-1]\n",
    "\n",
    "        tokenizer = Tokenizer(\n",
    "            chunk_overlap=self._chunk_overlap,\n",
    "            tokens_per_chunk=self.tokens_per_chunk,\n",
    "            decode=self.tokenizer.decode,\n",
    "            encode=encode_strip_start_and_stop_token_ids,\n",
    "        )\n",
    "\n",
    "        return self.split_text_on_tokens(text=text, tokenizer=tokenizer)\n",
    "    \n",
    "    def split_text_on_tokens(self, *, text: str, tokenizer: Tokenizer) -> List[str]:\n",
    "        \"\"\"Split incoming text and return chunks using tokenizer.\"\"\"\n",
    "        splits: List[str] = []\n",
    "        input_ids = self._encode(text)\n",
    "        start_idx = 0\n",
    "        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "        chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        while start_idx < len(input_ids):\n",
    "            splits.append(self._decode(chunk_ids))\n",
    "            if cur_idx == len(input_ids):\n",
    "                break\n",
    "            start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n",
    "            cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n",
    "            chunk_ids = input_ids[start_idx:cur_idx]\n",
    "        return splits\n",
    "    \n",
    "    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents.\"\"\"\n",
    "        texts, metadatas = [], []\n",
    "        for doc in documents:\n",
    "            texts.append(doc.page_content)\n",
    "            metadatas.append(doc.metadata)\n",
    "        return self.create_documents(texts, metadatas=metadatas)\n",
    "    \n",
    "    def create_documents(\n",
    "        self, texts: List[str], metadatas: Optional[List[dict]] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"Create documents from a list of texts.\"\"\"\n",
    "        _metadatas = metadatas or [{}] * len(texts)\n",
    "        documents = []\n",
    "        for i, text in tqdm(enumerate(texts), total = len(texts), desc = \"Split documents by tokens\"):\n",
    "            index = 0\n",
    "            previous_chunk_len = 0\n",
    "            for chunk in self.split_text(text):\n",
    "                metadata = copy.deepcopy(_metadatas[i])\n",
    "                if self._add_start_index:\n",
    "                    offset = index + previous_chunk_len - self._chunk_overlap\n",
    "                    index = text.find(chunk, max(0, offset))\n",
    "                    metadata[\"start_index\"] = index\n",
    "                    previous_chunk_len = len(chunk)\n",
    "                new_doc = Document(page_content=chunk, metadata=metadata)\n",
    "                documents.append(new_doc)\n",
    "        return documents\n",
    "\n",
    "\n",
    "class CustomSemanticChunker(SemanticChunker):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer = CostumTokenizer(model_name = self.embeddings,\n",
    "                                    tokens_per_chunk = 508,\n",
    "                                    chunk_overlap = 170)\n",
    "    \n",
    "    def split_documents(self, documents: Iterable[Document]) -> List[Document]:\n",
    "        \"\"\"Split documents.\"\"\"\n",
    "        texts, metadatas = [], []\n",
    "        with tqdm(total = len(documents), desc = \"Split documents\") as pbar:\n",
    "            for doc in documents:\n",
    "                texts.append(doc.page_content)\n",
    "                metadatas.append(doc.metadata)\n",
    "                pbar.update(1)\n",
    "            return self.create_documents(texts, metadatas=metadatas)\n",
    "        \n",
    "    def create_documents(\n",
    "            self, texts: List[str], metadatas: Optional[List[dict]] = None\n",
    "        ) -> List[Document]:\n",
    "            \"\"\"Create documents from a list of texts.\"\"\"\n",
    "            _metadatas = metadatas or [{}] * len(texts)\n",
    "            documents = []\n",
    "            error_counter = 0\n",
    "            with tqdm(total = len(texts), desc = \"Create documents\", position=0, leave=True) as pbar:\n",
    "                for i, text in enumerate(texts):\n",
    "                    index = -1\n",
    "                    for chunk in self.split_text(text):\n",
    "                        metadata = copy.deepcopy(_metadatas[i])\n",
    "                        if self._add_start_index:\n",
    "                            index = text.find(chunk, index + 1)\n",
    "                            metadata[\"start_index\"] = index\n",
    "                        new_doc = Document(page_content=chunk, metadata=metadata)\n",
    "                        documents.extend(self.tokenizer.split_documents([new_doc]))\n",
    "                    pbar.update(1)\n",
    "            print(\"Errors: \", str(error_counter))\n",
    "            return documents\n",
    "    def split_text(\n",
    "        self,\n",
    "        text: str,\n",
    "    ) -> List[str]:     \n",
    "        \n",
    "        d_nlp = nlp(text)\n",
    "        single_sentences_list = [sent.text.strip() for sent in d_nlp.sents]\n",
    "        if len(single_sentences_list) == 1:\n",
    "            return single_sentences_list\n",
    "        distances, sentences = self._calculate_sentence_distances(single_sentences_list)\n",
    "        if distances is None or len(distances) ==0:\n",
    "            return []\n",
    "        if self.number_of_chunks is not None:\n",
    "            breakpoint_distance_threshold = self._threshold_from_clusters(distances)\n",
    "        else:\n",
    "            breakpoint_distance_threshold = self._calculate_breakpoint_threshold(\n",
    "                distances\n",
    "            )\n",
    "\n",
    "        indices_above_thresh = [\n",
    "            i for i, x in enumerate(distances) if x > breakpoint_distance_threshold\n",
    "        ]\n",
    "\n",
    "        chunks = []\n",
    "        start_index = 0\n",
    "        # Iterate through the breakpoints to slice the sentences\n",
    "        for index in indices_above_thresh:\n",
    "            # The end index is the current breakpoint\n",
    "            end_index = index\n",
    "\n",
    "            # Slice the sentence_dicts from the current start index to the end index\n",
    "            group = sentences[start_index : end_index + 1]\n",
    "            combined_text = \" \".join([d[\"sentence\"] for d in group])\n",
    "            chunks.append(combined_text)\n",
    "\n",
    "            # Update the start index for the next group\n",
    "            start_index = index + 1\n",
    "\n",
    "        # The last group, if any sentences remain\n",
    "        if start_index < len(sentences):\n",
    "            combined_text = \" \".join([d[\"sentence\"] for d in sentences[start_index:]])\n",
    "            chunks.append(combined_text)\n",
    "        return chunks\n",
    "        \n",
    "    def _calculate_sentence_distances(\n",
    "            self, single_sentences_list: List[str]\n",
    "        ) -> Tuple[List[float], List[dict]]:\n",
    "        \"\"\"Split text into multiple components.\"\"\"\n",
    "        single_sentences_list = [sent\n",
    "            for sentence in single_sentences_list \n",
    "            for sent in self.tokenizer.split_text_on_tokens(text = sentence, tokenizer = self.tokenizer)]\n",
    "        _sentences = [\n",
    "            {\"sentence\": x, \"index\": i} for i, x in enumerate(single_sentences_list)\n",
    "        ]\n",
    "        \n",
    "        sentences = combine_sentences(_sentences, self.buffer_size)\n",
    "        tmp_list = []\n",
    "        for sent in sentences:\n",
    "            tmp_list.extend(self.tokenizer.split_text_on_tokens(text = sent[\"combined_sentence\"], tokenizer = self.tokenizer))\n",
    "        sentences = [{\"sentence\": sentence,\n",
    "                      \"index\": idx,\n",
    "                     \"combined_sentence\": sentence\n",
    "                     } for idx, sentence in enumerate(tmp_list)]\n",
    "        embeddings = self.embeddings.embed_documents(\n",
    "            [x[\"combined_sentence\"] for x in sentences]\n",
    "        )\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence[\"combined_sentence_embedding\"] = embeddings[i]\n",
    "    \n",
    "        sentences = [sent \n",
    "                     for sent in sentences \n",
    "                     if sent[\"combined_sentence_embedding\"] != None \n",
    "                     and len(sent[\"combined_sentence_embedding\"])>0\n",
    "                     and isinstance(sent[\"combined_sentence_embedding\"], list)\n",
    "                    ]\n",
    "        return calculate_cosine_distances(sentences)\n",
    "    \n",
    "def combine_sentences(sentences: List[dict], buffer_size: int = 1) -> List[dict]:\n",
    "    \"\"\"Combine sentences based on buffer size.\n",
    "\n",
    "    Args:\n",
    "        sentences: List of sentences to combine.\n",
    "        buffer_size: Number of sentences to combine. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        List of sentences with combined sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Go through each sentence dict\n",
    "    for i in range(len(sentences)):\n",
    "        # Create a string that will hold the sentences which are joined\n",
    "        combined_sentence = \"\"\n",
    "\n",
    "        # Add sentences before the current one, based on the buffer size.\n",
    "        for j in range(i - buffer_size, i):\n",
    "            # Check if the index j is not negative\n",
    "            # (to avoid index out of range like on the first one)\n",
    "            if j >= 0:\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += sentences[j][\"sentence\"] + \" \"\n",
    "\n",
    "        # Add the current sentence\n",
    "        combined_sentence += sentences[i][\"sentence\"]\n",
    "\n",
    "        # Add sentences after the current one, based on the buffer size\n",
    "        for j in range(i + 1, i + 1 + buffer_size):\n",
    "            # Check if the index j is within the range of the sentences list\n",
    "            if j < len(sentences):\n",
    "                # Add the sentence at index j to the combined_sentence string\n",
    "                combined_sentence += \" \" + sentences[j][\"sentence\"]\n",
    "\n",
    "        # Then add the whole thing to your dict\n",
    "        #Split after 512 tokens\n",
    "        sentences[i][\"combined_sentence\"] = combined_sentence\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def calculate_cosine_distances(sentences: List[dict]) -> Tuple[List[float], List[dict]]:\n",
    "    \"\"\"Calculate cosine distances between sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences: List of sentences to calculate distances for.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of distances and sentences.\n",
    "    \"\"\"\n",
    "    distances = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        embedding_current = sentences[i][\"combined_sentence_embedding\"]\n",
    "        embedding_next = sentences[i + 1][\"combined_sentence_embedding\"]\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
    "\n",
    "        # Convert to cosine distance\n",
    "        distance = 1 - similarity\n",
    "\n",
    "        # Append cosine distance to the list\n",
    "        distances.append(distance)\n",
    "\n",
    "        # Store distance in the dictionary\n",
    "        sentences[i][\"distance_to_next\"] = distance\n",
    "\n",
    "    # Optionally handle the last sentence\n",
    "    # sentences[-1]['distance_to_next'] = None  # or a default value\n",
    "\n",
    "    return distances, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "570c0ebc-5738-4dc5-8c5e-103b02a3602d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on CUDA:  True\n"
     ]
    }
   ],
   "source": [
    "# Init embeddings models\n",
    "embeddings_model = CustomHuggingfaceEmbeddings(model_id = EMBD_MODEL, \n",
    "                                           api_key = AUTH_TOKEN, \n",
    "                                           model_kwargs = {\"device\": DEVICE})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110811f1-def8-4d0a-99ac-82e99db10228",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de335123-066a-45a8-85d0-7ce2c0c9c4da",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "semantic_splitter = CustomSemanticChunker(embeddings_model, buffer_size = 2, breakpoint_threshold_type = \"percentile\", breakpoint_threshold_amount = 90)\n",
    "splitted_data = semantic_splitter.split_documents(data)\n",
    "#splitted_data = semantic_splitter.split_documents(test_doc)\n",
    "len(splitted_data) - len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26572435-2f3e-4404-a8d8-f3489978646f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "embd_docs_semantic_split = [(embeddings_model.embed_query(doc.page_content), doc) for doc in tqdm(splitted_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "ea76d6e3-a171-4edf-aed0-ec7b9c10f707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(\"embd_docs_semantic_split\", 'wb') as fp:\n",
    "          pickle.dump(embd_docs_semantic_split, fp)\n",
    "          print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abcf28f-9585-4ca6-99d3-e69c662f81bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Token Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305408c3-71f0-4ac9-9bbf-ad284df03584",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = CostumTokenizer(model_name = EMBD_MODEL,\n",
    "                                 #length_function = splitting_func,\n",
    "                                tokens_per_chunk = 128,\n",
    "                                chunk_overlap = 42\n",
    "                                )\n",
    "# Split each document\n",
    "splitted_data = text_splitter.split_documents(data)\n",
    "len(splitted_data)-len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a06032aa-863f-4c6b-b11f-c5576a5cdf34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 539429/539429 [5:11:55<00:00, 28.82it/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "539429"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_docs_128_42 = [(embeddings_model.embed_query(doc.page_content), doc) for doc in tqdm(splitted_data)]\n",
    "len(embd_docs_128_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "4c7ff554-adce-4d8d-b463-a6ec9e8356fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(\"embd_docs_128_42\", 'wb') as fp:\n",
    "          pickle.dump(embd_docs_128_42, fp)\n",
    "          print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "2e782100-9b5d-4a8b-9a62-746ccfdcd4ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59938/59938 [14:20<00:00, 69.68it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "211654"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = CostumTokenizer(model_name = EMBD_MODEL,\n",
    "                                 #length_function = splitting_func,\n",
    "                                tokens_per_chunk = 256,\n",
    "                                chunk_overlap = 85\n",
    "                                )\n",
    "# Split each document\n",
    "splitted_data = text_splitter.split_documents(data)\n",
    "len(splitted_data)-len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d0200d31-1999-4d35-85f0-691b204e078b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 271592/271592 [4:29:36<00:00, 16.79it/s]   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "271592"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_docs_256_85 = [(embeddings_model.embed_query(doc.page_content), doc) for doc in tqdm(splitted_data)]\n",
    "len(embd_docs_256_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab0b77e-cc5d-4809-b760-2d232292563a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"embd_docs_256_85\", 'wb') as fp:\n",
    "          pickle.dump(embd_docs_256_85, fp)\n",
    "          print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d085e-95a5-45d5-b99a-29db4188e41d",
   "metadata": {},
   "source": [
    "### Load ready-to-use Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94983033-d39c-49d7-ad30-066c15dfd5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271592"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_docs_256_85 = None\n",
    "with open(\"embd_docs_256_85\", 'rb') as fp:\n",
    "    embd_docs_256_85 = pickle.load(fp)\n",
    "len(embd_docs_256_85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "855ce088-1ce5-4e56-af84-aa0879463596",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "539429"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_docs_128_42 = None\n",
    "with open(\"embd_docs_128_42\", 'rb') as fp:\n",
    "    embd_docs_128_42 = pickle.load(fp)\n",
    "len(embd_docs_128_42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0164a3a-ea83-4a95-92f9-c597e91edc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "embd_docs_semantic_split = None\n",
    "with open(\"embd_docs_semantic_split\", 'rb') as fp:\n",
    "    embd_docs_semantic_split = pickle.load(fp)\n",
    "len(embd_docs_semantic_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d2fa7f-ad1a-42a7-a2bb-af60327856a0",
   "metadata": {},
   "source": [
    "# Advanced RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c671312b-021e-43d8-833b-863382b843e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AdvancedRAG():\n",
    "    \n",
    "    def __init__(self, generation_model = None, embeddings_model = None, vectorstore = None, use_retriever = None):\n",
    "        if generation_model != None:\n",
    "            self.k = 4\n",
    "            self.huggingface_api_key = \"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\"\n",
    "            self.device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "            self.generation_pipeline = self.initialize_generation_pipeline(generation_model)\n",
    "            self.embeddings_model = self.initialize_embeddings_model(embeddings_model)\n",
    "            self.vectorstore = self.initializeVectorStore(vectorstore = [\"time_weigthed\"])\n",
    "            self.vectorstore = self.initializeVectorStore(vectorstore)\n",
    "            self.docs = self.db_to_docs()\n",
    "            self.documents = [doc for _, doc in self.docs]\n",
    "            self.use_retriever = self.initialize_retriever(use_retriever)\n",
    "            self.prompt_template = None\n",
    "            self.retrieving_methods = use_retriever\n",
    "            self.cached_stop_words = stopwords.words(\"german\")\n",
    "            self.test_log = []\n",
    "            self.collect = False\n",
    "            self.gliner_model = GlinerNER()\n",
    "            #self.gliner_m = GLiNER.from_pretrained(\"urchade/gliner_base\")\n",
    "            self.gliner_labels = [\"date\",\"time\", \"club\", \"league\"]\n",
    "            \n",
    "        \n",
    "        \n",
    "    def initialize_generation_pipeline(self, generation_model):\n",
    "        if generation_model != None:\n",
    "            model_id = generation_model\n",
    "            auth_token = self.huggingface_api_key\n",
    "            bnb_config = transformers.BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type='nf4',\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.bfloat16\n",
    "            )\n",
    "\n",
    "            model_config = transformers.AutoConfig.from_pretrained(\n",
    "                model_id,\n",
    "                output_hidden_states=True,\n",
    "                use_auth_token=auth_token,\n",
    "            )\n",
    "\n",
    "\n",
    "            # Load model\n",
    "            auth_token = \"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\"  # The authorization code is insible to the public\n",
    "            gpt_tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=auth_token, device = \"auto\")\n",
    "            gpt_model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                                        trust_remote_code=True,\n",
    "                                                        config=model_config,\n",
    "                                                        quantization_config=bnb_config,\n",
    "                                                        device_map=\"auto\",\n",
    "                                                        use_auth_token=auth_token\n",
    "                                                        )\n",
    "\n",
    "            generation_pipeline = transformers.pipeline(\n",
    "                model=gpt_model,\n",
    "                tokenizer=gpt_tokenizer,\n",
    "                return_full_text=True,  # langchain expects the full text\n",
    "                task='text-generation',\n",
    "                temperature=0.3,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "                max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "                repetition_penalty=1.2,  # without this output begins repeating\n",
    "            )\n",
    "\n",
    "            llm = HuggingFacePipeline(pipeline=generation_pipeline)\n",
    "            return llm\n",
    "    \n",
    "    def from_loaded_models(self, generation_pipeline, embeddings_model, vectorstore, use_retriever):\n",
    "        self.k = 40\n",
    "        self.huggingface_api_key = \"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\"\n",
    "        self.device = f'cuda:{torch.cuda.current_device()}' if torch.cuda.is_available() else 'cpu'\n",
    "        self.generation_pipeline = generation_pipeline\n",
    "        self.embeddings_model = embeddings_model\n",
    "        \n",
    "        #self.vectorstore = self.initializeVectorStore(vectorstore = [\"time_weigthed\"])\n",
    "        self.vectorstore = self.initializeVectorStore(vectorstore = vectorstore)\n",
    "        \n",
    "        self.docs = self.db_to_docs()\n",
    "        self.documents = [doc for _, doc in self.docs]\n",
    "        \n",
    "        self.use_retriever = self.initialize_retriever(use_retriever)\n",
    "        self.prompt_template = None\n",
    "        self.retrieving_methods = use_retriever\n",
    "        self.cached_stop_words = stopwords.words(\"german\")\n",
    "        self.test_log = []\n",
    "        self.collect = False\n",
    "        self.gliner_model = GlinerNER()\n",
    "        #self.gliner_m = GLiNER.from_pretrained(\"urchade/gliner_base\")\n",
    "        self.gliner_labels = [\"date\",\"time\", \"club\", \"league\"]\n",
    "        \n",
    "    def db_to_docs(self):\n",
    "        documents = self.vectorstore.get(include = [\"metadatas\", \"documents\", \"embeddings\"])        \n",
    "        docs = [(torch.tensor(documents[\"embeddings\"][idx]), Document(page_content = documents[\"documents\"][idx], metadata=documents[\"metadatas\"][idx])) for idx, doc in enumerate(documents[\"documents\"])]\n",
    "        return docs\n",
    "    \n",
    "    def set_documents(self, documents):\n",
    "        self.docs = [(self.embeddings_model.embed_query(doc.page_content), doc) for doc in tqdm(documents, desc=\"set_documents\")]\n",
    "        \n",
    "    def initialize_embeddings_model(self, embeddings_model):\n",
    "        return CustomHuggingfaceEmbeddings(model_id = embeddings_model, \n",
    "                                           api_key = self.huggingface_api_key, \n",
    "                                           model_kwargs = {\"device\": self.device})\n",
    "    \n",
    "    def initialize_retriever(self, retriever_methods: List[str]): # [\"similartiy\", \"keyword\", \"time_weighted\"]\n",
    "        retrievers = {}\n",
    "        for method in retriever_methods:\n",
    "            if method == \"similarity\":\n",
    "                retrievers[\"similarity\"] = self.vectorstore.as_retriever(search_type = \"similarity\", \n",
    "                                                                    search_kwargs = { \"k\" : self.k,\n",
    "                                                                                     #\"filter\":{\"published_date\": {\"eq\": [}\n",
    "                                                                                     #\"filter\": lambda d: \"2023\" in d[\"published_date\"]\n",
    "                                                                    #search_kwargs={'filter':{'keywords': {'in': ['asthma', 'appointment']}}}\n",
    "                                                                                    })\n",
    "                                                                                     \n",
    "            if method == \"keyword\":\n",
    "                retrievers[\"keyword\"] = BM25Retriever.from_documents(self.documents, verbose = True, k = 10)\n",
    "\n",
    "\n",
    "            if method == \"time_weigthed\":\n",
    "                retrievers[\"time_weigthed\"] = CustomTimeWeightedVectorStoreRetriever(vectorstore=self.vectorstore, \n",
    "                                                                                decay_rate=0.001, \n",
    "                                                                                k = self.k\n",
    "                                                                               )\n",
    "                \n",
    "        return retrievers\n",
    "    \n",
    "    def initializeVectorStore(self, vectorstore):\n",
    "        if isinstance(vectorstore, str):\n",
    "            return Chroma(persist_directory=vectorstore, embedding_function=self.embeddings_model)\n",
    "        else:\n",
    "            return vectorstore\n",
    "    \n",
    "    def remove_stopwords(self,content):\n",
    "        return ' '.join([word for word in content.split() if word not in self.cached_stop_words])\n",
    "    \n",
    "    def costum_similartiy_search(self, query):\n",
    "        embd_query = self.embeddings_model.embed_query(query)\n",
    "        tmp_docs = []\n",
    "        res = [(util.cos_sim(embd_query, embd)[0], doc) for embd, doc in self.tmp_docs]\n",
    "        print(res[:2])\n",
    "        res = sorted(res, reverse = True, key=lambda x: x[0])\n",
    "        print(res[:2])\n",
    "        res = [doc for _, doc in res]\n",
    "        return res\n",
    "    \n",
    "    def collect_test(self, \n",
    "                  query,\n",
    "                  inference,\n",
    "                  context, \n",
    "                  retrieve_methods, \n",
    "                  multi_query, \n",
    "                  remove_stopwords,\n",
    "                  execution_time,\n",
    "                  documents_in_scope,\n",
    "                  **kwargs\n",
    "                 ):\n",
    "        \n",
    "        test_dict = {\n",
    "            \"query\": query,\n",
    "            \"inference\": inference,\n",
    "            \"context\": [(doc, score) for doc, score in context],\n",
    "            \"retrieve_methods\": retrieve_methods,\n",
    "            \"multi_query\": multi_query,\n",
    "            \"remove_stopwords\": remove_stopwords,\n",
    "            \"inference_time\": execution_time,\n",
    "            \"documents_in_scope\": documents_in_scope\n",
    "        }\n",
    "        final_test_dict = {**test_dict, **kwargs}\n",
    "        self.test_log.append(final_test_dict)\n",
    "    \n",
    "    def save_test(self, path):\n",
    "        with open(path, 'wb') as fp:\n",
    "          pickle.dump(self.test_log, fp)\n",
    "          print(\"Done\")\n",
    "        \n",
    "    def date_processing(self, extracted_entities):\n",
    "        reg_dates = extracted_entities\n",
    "        result = [search_dates(item[\"text\"]) for item in reg_dates if item[\"label\"] in [\"date\"]]\n",
    "        if len(result)>0:\n",
    "            result = [item for item in result if item is not None]\n",
    "            if len(result) >0:\n",
    "                result = result[0]\n",
    "        return result\n",
    "    \n",
    "    def extract_ent_values(self, extracted_entities, scope_ent):\n",
    "        return [item[\"text\"] for item in extracted_entities if item[\"label\"] in [scope_ent]]\n",
    "\n",
    "    def extract_club(self, extracted_entities):\n",
    "        return [item[\"text\"] for item in extracted_entities if item[\"label\"] in [\"club\"]]\n",
    "    \n",
    "    def invoke(self, \n",
    "               query, \n",
    "               multi_query = True, \n",
    "               remove_stopwords = False, \n",
    "               m_retrieve_methods = None, \n",
    "               pre_filtering = True,\n",
    "               **kwargs):\n",
    "        split_inference = None\n",
    "        # Set k for top-k documents\n",
    "        k = 40\n",
    "        # Set start_time to calculate the total inference time\n",
    "        start_time = time.perf_counter()\n",
    "        # Initialize a list for queries, will use in the further process\n",
    "        queries = []\n",
    "        # Set the required dateformat to parse dates in the query\n",
    "        date_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "        \n",
    "        if m_retrieve_methods is not None:\n",
    "            print(\"Set retrieving methods\")\n",
    "            self.tmp_retrieving_methods = m_retrieve_methods\n",
    "        else:\n",
    "            self.tmp_retrieving_methods = self.retrieving_methods\n",
    "\n",
    "        date_text = None \n",
    "        date_time = None \n",
    "        club = None\n",
    "        league = None\n",
    "        \n",
    "        if pre_filtering is True:\n",
    "            \n",
    "            extracted_entities = self.gliner_model.predict_tags(query, verbose = True)\n",
    "            result_date_processing = self.date_processing(extracted_entities)\n",
    "\n",
    "            print(\"\\nResult_date_processing:\\n\", str(result_date_processing))\n",
    "            if result_date_processing is not None and len(result_date_processing) > 0 :\n",
    "                date_text, date_time = result_date_processing[0]\n",
    "\n",
    "            club = self.extract_ent_values(extracted_entities, \"club\")\n",
    "            league = self.extract_ent_values(extracted_entities, \"league\")        \n",
    "        \n",
    "        if not hasattr(self, \"tmp_docs\"):\n",
    "                print(\"has no tmp_docs\")\n",
    "                self.tmp_docs = self.docs\n",
    "                print(\"Length of tmp_docs: \", str(len(self.tmp_docs)))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # If datetime is not None\n",
    "        if date_time and isinstance(date_time, datetime):\n",
    "            #search_res = [doc for doc in self.documents if date_time[0] <= datetime.strptime(doc.metadata[\"published_date\"], date_format)]\n",
    "            search_res = [(embd, doc) for embd, doc in self.tmp_docs if date_time <= datetime.strptime(doc.metadata[\"published_date\"], date_format)]\n",
    "            if len(search_res) > 0:\n",
    "                self.tmp_docs = search_res.copy()\n",
    "            print(\"After datetime: \", len(self.tmp_docs))\n",
    "        # If an entity league is in the query\n",
    "        \n",
    "        if league and league is not None:\n",
    "            if not hasattr(self, \"tmp_docs\"):\n",
    "                print(\"has no tmp_docs\")\n",
    "                self.tmp_docs = self.docs\n",
    "                print(\"Initial len: \", len(self.tmp_docs))\n",
    "            for l in league:\n",
    "                #league = \"1. Bundesliga\"\n",
    "                pattern = r\"(\\d+)\\.Bundesliga\"\n",
    "                match = re.search(pattern, l, re.IGNORECASE)\n",
    "                if match:\n",
    "                    #league = league.group()\n",
    "                    l = match.group(1) + \".Bundesliga\"\n",
    "                    print(\"Extracted leauge:\", l)\n",
    "                else:\n",
    "                    print(\"Word not found.\")\n",
    "                \n",
    "                search_res = [(embd, doc) for embd, doc in self.tmp_docs if l.lower() in doc.metadata[\"news_keywords\"].lower()]\n",
    "                if len(search_res) > 0:\n",
    "                    self.tmp_docs = search_res.copy()\n",
    "                print(\"After League: \", len(self.tmp_docs))\n",
    "                print(l.lower())\n",
    "        # If an entity club is in the query\n",
    "        if club and club is not None:\n",
    "            if not hasattr(self, \"tmp_docs\"):\n",
    "                print(\"has no tmp_docs\")\n",
    "                self.tmp_docs = self.docs\n",
    "            for c in club:\n",
    "                search_res = [(embd, doc) for embd, doc in self.tmp_docs if c.lower() in doc.metadata[\"news_keywords\"].lower()]\n",
    "                if len(search_res) > 0:\n",
    "                    self.tmp_docs = search_res.copy()\n",
    "                print(\"After Club: \", len(self.tmp_docs))\n",
    "                print(c.lower())\n",
    "        \n",
    "        if not hasattr(self, \"tmp_docs\"):\n",
    "                print(\"has no tmp_docs\")\n",
    "                self.tmp_documents = self.docs\n",
    "        else:\n",
    "            self.tmp_documents = [doc for _, doc in self.tmp_docs]               \n",
    "        \n",
    "        # Removing stopwords if set by the user\n",
    "        if remove_stopwords:\n",
    "            if not hasattr(self, \"tmp_docs\"):\n",
    "                print(\"has no tmp_docs\")\n",
    "                self.tmp_docs = self.docs\n",
    "            # Remove stopwords\n",
    "            self.tmp_docs = [\n",
    "                Document(page_content=self.remove_stopwords(doc.page_content), metadata=doc.metadata) \n",
    "                for embd, doc in tqdm(self.tmp_docs, desc = \"Remove Stopwords\") \n",
    "            ]\n",
    "            \n",
    "            # Recalculate the embeddings\n",
    "            self.tmp_docs = [\n",
    "                (self.embeddings_model.embed_query(doc.page_content), doc) for doc in tqdm(self.tmp_docs)\n",
    "            ]\n",
    "            \n",
    "            # Write back docs to tmp_documents\n",
    "            self.tmp_documents = [doc for _, doc in self.tmp_docs]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # A dictionary for retrievers\n",
    "        retrievers = dict()\n",
    "        # A list for list of the relevant documents, retrievers/rankings append the result to this list \n",
    "        retrieved_results = []\n",
    "        result_context = []\n",
    "        \n",
    "        # For time-weigthed retrieving\n",
    "        if date_time is None or isinstance(date_time, datetime):\n",
    "            print(\"No datetime found.\")\n",
    "            date_time = datetime.now()\n",
    "        \n",
    "        # Check if self_tmp_documents is greater than 0\n",
    "        if len(self.tmp_documents) > 0:\n",
    "            # If multi_qurey\n",
    "            if multi_query:\n",
    "                # Generates queries\n",
    "                queries.extend(self.generate_queries(query))\n",
    "                # Append queries to list\n",
    "                queries.append(query)\n",
    "            else:\n",
    "                # If multi_query is false, than put the original query in a list for futher processing\n",
    "                queries = [query]\n",
    "\n",
    "            # Loop over queries and process rankings\n",
    "            for q in queries:\n",
    "\n",
    "                # Ranking by BM25\n",
    "                if \"keyword\" in self.tmp_retrieving_methods:\n",
    "                    retrievers[\"keyword\"] = BM25Retriever.from_documents(self.tmp_documents, verbose = True, k = k)\n",
    "                    #retrieved_results.append(self.use_retriever[\"keyword\"].get_relevant_documents(q))\n",
    "                    retrieved_results.append(retrievers[\"keyword\"].get_relevant_documents(q))\n",
    "\n",
    "                # Ranking by cosine similaritry\n",
    "                if \"similarity\" in self.tmp_retrieving_methods:\n",
    "                    retrieved_results.append(self.costum_similartiy_search(q))\n",
    "\n",
    "                # Ranking by time-delta to named date in query.\n",
    "                if \"time_weighted\" in self.tmp_retrieving_methods and isinstance(date_time, datetime):\n",
    "                    print(\"in time-weighted\")\n",
    "                    retrievers[\"time_weighted\"] = CustomTimeWeightedVectorStoreRetriever(vectorstore=self.vectorstore, \n",
    "                                                                                    decay_rate=0.01, \n",
    "                                                                                    k = k\n",
    "                                                                                   )\n",
    "                    time_ret = [(retrievers[\"time_weighted\"]._get_combined_score(doc, date_time), doc) for doc in self.tmp_documents]  # vorher über documents itariert\n",
    "                    time_ret = sorted(time_ret, reverse = True, key=lambda x: x[0])\n",
    "                    retrieved_results.append([doc for _, doc in time_ret]) \n",
    "\n",
    "                # Ranking by news_keywords in metadata\n",
    "                if \"metadata\" in self.tmp_retrieving_methods:\n",
    "                    metadata_retrieved = []\n",
    "                    news_keywords = []\n",
    "                    for doc in self.tmp_documents:\n",
    "                        score = 0\n",
    "                        news_keywords = doc.metadata[\"news_keywords\"].lower()\n",
    "                        news_keywords = news_keywords.split(\",\")\n",
    "                        if \"leverkusen\" in news_keywords:\n",
    "                            score += 1/len(news_keywords)\n",
    "                        if \"bundesliga\" in news_keywords:\n",
    "                            score += 1/len(news_keywords)\n",
    "                        metadata_retrieved.append((score, doc))\n",
    "\n",
    "                    sort_metadata_retrieved = sorted(metadata_retrieved, reverse = True, key=lambda x: x[0])\n",
    "                    retrieved_results.append([doc for _, doc in sort_metadata_retrieved])\n",
    "\n",
    "            # ExecuteReciprocal Rank Fusion\n",
    "            result_context = self.reciprocal_rank_fusion(retrieved_results)\n",
    "            # Sort the RRF results\n",
    "            result_context = sorted(result_context, reverse = True, key = lambda x: float(x[1]))[:6]\n",
    "        \n",
    "        # Long-context re-ranking, if context length is greater than three\n",
    "        if len(result_context) > 3:\n",
    "            last = result_context.pop(3)\n",
    "            result_context.append(last)\n",
    "        \n",
    "        # if there isn't a promp template, then set it\n",
    "        if self.prompt_template == None:\n",
    "            template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "            If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "            Use six sentences maximum and keep the answer as concise as possible.\n",
    "            Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "            Context: {context}\n",
    "\n",
    "            Question: {question}\n",
    "\n",
    "            Helpful Answer:\"\"\"\n",
    "            \n",
    "            # Remove \\t and \\n and set the template as ChatPromptTemplate\n",
    "            self.prompt_template = ChatPromptTemplate.from_template(template.replace(\"\\t\", \"\").replace(\"\\n\", \" \"))\n",
    "        if isinstance(self.prompt_template, str):\n",
    "            split_inference = True\n",
    "            self.prompt_template = ChatPromptTemplate.from_template(self.prompt_template.replace(\"\\t\", \"\").replace(\"\\n\", \" \"))\n",
    "        \n",
    "        # Join the context together\n",
    "        context = \" \".join([doc.page_content for doc, score in result_context])\n",
    "        # Return docs out of result_context\n",
    "        documents = [doc for doc, score in result_context]\n",
    "        # Set Chain\n",
    "        chain = (\n",
    "            self.prompt_template\n",
    "            |self.generation_pipeline\n",
    "            |StrOutputParser()\n",
    "        )\n",
    "        # Pass the original query and the related context to chain\n",
    "        inference = chain.invoke({\"question\": query,\n",
    "                                  \"context\" : context})\n",
    "        \n",
    "        if isinstance(self.split_inference, str):\n",
    "            inference = inference.split(self.split_inference)[-1].strip()\n",
    "        # Calculate inference time\n",
    "        end_time = time.perf_counter()\n",
    "        execution_time = end_time - start_time\n",
    "        # Answers of model + Sources\n",
    "        print(\"\\n\"+\n",
    "            str(inference) + \"\\nSources:\\n\" + \n",
    "            \"\\n\".join(set([str(score) + \" - \" \n",
    "                           + doc.metadata[\"published_date\"] + \" \" \n",
    "                           + doc.metadata[\"source\"] for doc, score in result_context])))\n",
    "        # If collect true, then apoend log with this execution parameters and result\n",
    "        if self.collect:\n",
    "            self.collect_test( \n",
    "                  query,\n",
    "                  inference,\n",
    "                  result_context, \n",
    "                  self.tmp_retrieving_methods, \n",
    "                  multi_query, \n",
    "                  remove_stopwords,\n",
    "                  execution_time,\n",
    "                  #detected_entities\n",
    "                  len(self.tmp_documents),\n",
    "                  **kwargs\n",
    "                 )\n",
    "        del self.tmp_documents, self.tmp_docs\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join([doc.page_content for doc, score in docs])\n",
    "    \n",
    "    def generate_queries(self, query):\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"Sie sind ein hilfreicher Assistent, der mehrere Suchanfragen auf der Grundlage einer einzigen Eingabeabfrage erstellt.\"),\n",
    "            (\"user\", \"Generieren Sie mehrere Suchanfragen zu folgenden Themen: {original_query}\"),\n",
    "            (\"user\", \"OUTPUT (4 queries):\")\n",
    "        ])\n",
    "        \n",
    "        generate_queries = (\n",
    "            prompt | self.generation_pipeline | StrOutputParser() | (lambda x: [q.strip() for q in x.split(\"\\n\") if len(q.strip()) > 0])\n",
    "        )\n",
    "        questions = generate_queries.invoke({\"original_query\": query})       \n",
    "        regex_pattern = r'(?:\\d+\\.|\\w+\\s\\d+:)\\s*(.*?\\?)'\n",
    "        ques = []\n",
    "        for q in questions:\n",
    "            ques.extend(re.findall(regex_pattern, q))\n",
    "        return ques\n",
    "    \n",
    "    def format_docs(self, docs):\n",
    "        return \" \".join(doc.page_content for doc in docs)\n",
    "    def reciprocal_rank_fusion(self, results: list[list], k=60):\n",
    "        fused_scores = {}\n",
    "        for docs in results:\n",
    "            # Assumes the docs are returned in sorted order of relevance\n",
    "            for rank, doc in enumerate(docs):\n",
    "                doc_str = dumps(doc)\n",
    "                if doc_str not in fused_scores:\n",
    "                    fused_scores[doc_str] = 0\n",
    "                previous_score = fused_scores[doc_str]\n",
    "                fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "        reranked_results = [\n",
    "            (loads(doc), score)\n",
    "            for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        ]\n",
    "        return reranked_results\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6969d7f5-8b52-400a-bf24-4049981a3a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "german_prompt_template = \"\"\"Beantworte die Frage am Ende des Textes anhand der folgenden Informationen.\n",
    "Wenn du die Antwort nicht weißt, sage einfach, dass du es nicht wissen, versuche keine Antwort zu erfinden.\n",
    "Verfasse die Antwort so kurz wie möglich und solange wie nötig.\n",
    "Kontext: {context}\n",
    "Frage: {question}\n",
    "Hilfreiche Antwort:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19964260-1b2f-4643-8dbd-8fb975e6a174",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "rag = AdvancedRAG()\n",
    "rag.from_loaded_models(\n",
    "# from_loaded_models(self, generation_pipeline, embeddings_model, vectorstore, use_retriever):\n",
    "    generation_pipeline = llm,\n",
    "    embeddings_model = embeddings_model,\n",
    "    vectorstore = \"./token_splitted_db\",\n",
    "    use_retriever = [\"keyword\", \"similarity\"]\n",
    ")\n",
    "#print(\"self.docs[0]\", rag.docs[0])\n",
    "print(\"Len self.docs\", str(len(rag.docs)))\n",
    "#rag.docs = embd_docs_256_85\n",
    "rag.docs = embd_docs_128_42\n",
    "# print(\"self.docs[0]\", rag.docs[0])\n",
    "#rag.docs = embd_docs_semantic_split\n",
    "print(\"Len self.docs\", str(len(rag.docs)))\n",
    "rag.prompt_template = german_prompt_template\n",
    "rag.collect = True\n",
    "rag.split_inference = \"Hilfreiche Antwort:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e8c8d-e4d2-4bef-b123-8f9323484d6e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag.invoke(\"Wie hat Bayer Leverkusen am 2023-11-25 der Bundesliga-Hinrunde 2023 gespielt?\", \n",
    "    #\"Wie hat Leverkusen gegen Bremen in der Bundesliga-Hinrunde 2023 gespielt?\",\n",
    "    #\"Wie hat Bayer Leverkusen am 2023-11-25 der Bundesliga-Hinrunde 2023 gespielt?\", \n",
    "    #\"Wie hat Leverkusen gegen Bremen gespielt?\",\n",
    "           multi_query=True, \n",
    "           remove_stopwords = False, \n",
    "           m_retrieve_methods = [\"keyword\", \"similarity\", \"time_weighted\"],\n",
    "           pre_filtering = True,\n",
    "           kwargs = {\n",
    "            \"gen_model\": GEN_MODEL,\n",
    "            \"embeddings_model\": EMBD_MODEL,\n",
    "            \"chunk_strategie\": (128,42),\n",
    "            \"ground_truths\": \"Nett hier\",\n",
    "           }\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434ef0c5-aa4d-43d0-83f8-1a621de00e67",
   "metadata": {},
   "source": [
    "https://www.reviersport.de/fussball/1bundesliga/a599423---bundesliga-leverkusen-setzt-siegesserie-fort-dortmund-zeigt-moral.html\n",
    "\n",
    "Gegen Bremen, 3:0 für Leverkusen am 25.11.2023 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05748405-0ecb-4f1c-afb3-b71efc78b880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag.test_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc58d6b-e088-45d0-97e3-6c51cc22e03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['keyword'],\n",
       " ['similarity'],\n",
       " ['time_weighted'],\n",
       " ['metadata'],\n",
       " ['keyword', 'similarity'],\n",
       " ['keyword', 'time_weighted'],\n",
       " ['keyword', 'metadata'],\n",
       " ['similarity', 'time_weighted'],\n",
       " ['similarity', 'metadata'],\n",
       " ['time_weighted', 'metadata'],\n",
       " ['keyword', 'similarity', 'time_weighted'],\n",
       " ['keyword', 'similarity', 'metadata'],\n",
       " ['keyword', 'time_weighted', 'metadata'],\n",
       " ['similarity', 'time_weighted', 'metadata'],\n",
       " ['keyword', 'similarity', 'time_weighted', 'metadata']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "stuff = [\"keyword\", \"similarity\", \"time_weighted\", \"metadata\"]\n",
    "retriever_combination = []\n",
    "for L in range(len(stuff) + 1):\n",
    "    for subset in itertools.combinations(stuff, L):\n",
    "        retriever_combination.append(list(subset))\n",
    "retriever_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85d2b0df-b39a-403b-8354-837ba3dd7aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Ground_Truths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wie viele gelbe Karten gabs es im Spiel Werder...</td>\n",
       "      <td>Es gab für Bremen drei gelbe Karten und für Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Welcher Schiedsrichter hat die Begnung Bremen ...</td>\n",
       "      <td>Das Schiedsrichter-Gespann bestand aus Martin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wie viele gelbe Karten gabs es im Spiel Werder...</td>\n",
       "      <td>Es gab für Bremen drei gelbe Karten und für Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Welcher Schiedsrichter hat die Begnung Bremen ...</td>\n",
       "      <td>Das Schiedsrichter-Gespann bestand aus Martin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wie viele gelbe Karten gab es im Spiel Werder ...</td>\n",
       "      <td>Es gab für Bremen drei gelbe Karten und für Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Welcher Schiedsrichter hat die Begegnung Breme...</td>\n",
       "      <td>Das Schiedsrichter-Gespann bestand aus Martin ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question   \n",
       "0  Wie viele gelbe Karten gabs es im Spiel Werder...  \\\n",
       "1  Welcher Schiedsrichter hat die Begnung Bremen ...   \n",
       "2  Wie viele gelbe Karten gabs es im Spiel Werder...   \n",
       "3  Welcher Schiedsrichter hat die Begnung Bremen ...   \n",
       "4  Wie viele gelbe Karten gab es im Spiel Werder ...   \n",
       "5  Welcher Schiedsrichter hat die Begegnung Breme...   \n",
       "\n",
       "                                       Ground_Truths  \n",
       "0  Es gab für Bremen drei gelbe Karten und für Le...  \n",
       "1  Das Schiedsrichter-Gespann bestand aus Martin ...  \n",
       "2  Es gab für Bremen drei gelbe Karten und für Le...  \n",
       "3  Das Schiedsrichter-Gespann bestand aus Martin ...  \n",
       "4  Es gab für Bremen drei gelbe Karten und für Le...  \n",
       "5  Das Schiedsrichter-Gespann bestand aus Martin ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df= pd.read_csv(\"../Question_catalog.csv\", sep =\";\")\n",
    "df= pd.read_csv(\"../error_in_query.csv\", sep =\";\")\n",
    "questions = list(df[\"Question\"])\n",
    "questions\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbe6ad04-2982-40cd-b708-a9a3289e104d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wie viele gelbe Karten gabs es im Spiel Werder Bremen gegen Bayer Leverkusen am 2023-11-25?',\n",
       " 'Welcher Schiedsrichter hat die Begnung Bremen gegen Leverkusen am 2023-11-25 gepfiffen?',\n",
       " 'Wie viele gelbe Karten gabs es im Spiel Werder Bremen gegen Bayer Leverkusen am 2023-11-25?',\n",
       " 'Welcher Schiedsrichter hat die Begnung Bremen gegen Leverkusen am 2023-11-25 gepfiffen?',\n",
       " 'Wie viele gelbe Karten gab es im Spiel Werder Bremen gegen Bayer Leverkusen am 2023-11-25?',\n",
       " 'Welcher Schiedsrichter hat die Begegnung Bremen gegen Leverkusen am 2023-11-25 gepfiffen?']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74fc695d-d256-4946-8d46-49fe3db3ce07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = [None, embd_docs_256_85, embd_docs_128_42]\n",
    "# chunk_strategie_list = [(508,170),(256,85),(128,42)]\n",
    "#data = [embd_docs_semantic_split]\n",
    "#chunk_strategie_list = [\"semantic_chunk\"]\n",
    "data = [embd_docs_128_42]\n",
    "chunk_strategie_list = [(128,42)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e741d8a-f8c3-4ebd-b2ce-a59a3cbafab7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ch_id, d in tqdm(enumerate(data)):\n",
    "    chunk_strategie = chunk_strategie_list[ch_id]\n",
    "    if d is not None:\n",
    "            rag.docs = d\n",
    "    for idx, row in df.iterrows():\n",
    "        query = row[\"Question\"]\n",
    "        ground_truths = row[\"Ground_Truths\"]\n",
    "        \n",
    "        multi_query_flag = idx < 2\n",
    "                    \n",
    "        for retrieving_com in retriever_combination:\n",
    "            # rag.invoke(\"Wie hat Leverkusen am 2023-11-25 in der 1.Bundesliga gespielt?\", \n",
    "            #            multi_query=False, \n",
    "            #            remove_stopwords = False, \n",
    "            #            m_retrieve_methods = retrieving_com\n",
    "            #           )\n",
    "            for filter_flag in [True]:\n",
    "                rag.invoke(query, \n",
    "                           multi_query=multi_query_flag, \n",
    "                           remove_stopwords = False, \n",
    "                           m_retrieve_methods = retrieving_com,\n",
    "                           pre_filtering = filter_flag,\n",
    "                           kwargs = {\n",
    "                            \"ground_truths\": ground_truths,\n",
    "                            \"gen_model\": GEN_MODEL,\n",
    "                            \"embeddings_model\": EMBD_MODEL,\n",
    "                            \"chunk_strategie\": chunk_strategie            \n",
    "                           }\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d257c9d-1473-4a25-a168-c082fda3b4cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test_log = pd.DataFrame(rag.test_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34103136-9298-435a-97e3-15d8cf00ccc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>inference</th>\n",
       "      <th>context</th>\n",
       "      <th>retrieve_methods</th>\n",
       "      <th>multi_query</th>\n",
       "      <th>remove_stopwords</th>\n",
       "      <th>inference_time</th>\n",
       "      <th>documents_in_scope</th>\n",
       "      <th>kwargs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wie viele gelbe Karten gabs es im Spiel Werder...</td>\n",
       "      <td>Die Notrufnummer der New Yorker Polizei ist 91...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>44.417751</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Es gab für Bremen drei gelb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wie viele gelbe Karten gabs es im Spiel Werder...</td>\n",
       "      <td>2 Gelbe Karten gab es im Spiel Werder Bremen g...</td>\n",
       "      <td>[(page_content='leverkusen hat die tabellenfuh...</td>\n",
       "      <td>[keyword]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>25.888465</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Es gab für Bremen drei gelb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wie viele gelbe Karten gabs es im Spiel Werder...</td>\n",
       "      <td>2 Gelbe Karten gab es im Spiel Werder Bremen g...</td>\n",
       "      <td>[(page_content='leverkusen hat die tabellenfuh...</td>\n",
       "      <td>[similarity]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>25.412266</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Es gab für Bremen drei gelb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wie viele gelbe Karten gabs es im Spiel Werder...</td>\n",
       "      <td>2 Gelbe Karten gab es im Spiel Werder Bremen g...</td>\n",
       "      <td>[(page_content='leverkusen hat die tabellenfuh...</td>\n",
       "      <td>[time_weighted]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>40.063634</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Es gab für Bremen drei gelb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wie viele gelbe Karten gabs es im Spiel Werder...</td>\n",
       "      <td>2 Gelbe Karten gab es im Spiel Werder Bremen g...</td>\n",
       "      <td>[(page_content='leverkusen hat die tabellenfuh...</td>\n",
       "      <td>[metadata]</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>29.105342</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Es gab für Bremen drei gelb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Welcher Schiedsrichter hat die Begegnung Breme...</td>\n",
       "      <td>Brych</td>\n",
       "      <td>[(page_content='leverkusen hat die tabellenfuh...</td>\n",
       "      <td>[keyword, similarity, time_weighted]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11.310008</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Das Schiedsrichter-Gespann ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Welcher Schiedsrichter hat die Begegnung Breme...</td>\n",
       "      <td>Brych</td>\n",
       "      <td>[(page_content='leverkusen hat die tabellenfuh...</td>\n",
       "      <td>[keyword, similarity, metadata]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>5.404342</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Das Schiedsrichter-Gespann ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Welcher Schiedsrichter hat die Begegnung Breme...</td>\n",
       "      <td>Ich kann diese Information nicht beantworten, ...</td>\n",
       "      <td>[(page_content='leverkusen hat die tabellenfuh...</td>\n",
       "      <td>[keyword, time_weighted, metadata]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12.967905</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Das Schiedsrichter-Gespann ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Welcher Schiedsrichter hat die Begegnung Breme...</td>\n",
       "      <td>Felix Zwayer</td>\n",
       "      <td>[(page_content='saison weiter ungeschlagenen l...</td>\n",
       "      <td>[similarity, time_weighted, metadata]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>11.517587</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Das Schiedsrichter-Gespann ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Welcher Schiedsrichter hat die Begegnung Breme...</td>\n",
       "      <td>Felix Zwayer</td>\n",
       "      <td>[(page_content='leverkusen hat die tabellenfuh...</td>\n",
       "      <td>[keyword, similarity, time_weighted, metadata]</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12.024930</td>\n",
       "      <td>12</td>\n",
       "      <td>{'ground_truths': 'Das Schiedsrichter-Gespann ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query   \n",
       "0   Wie viele gelbe Karten gabs es im Spiel Werder...  \\\n",
       "1   Wie viele gelbe Karten gabs es im Spiel Werder...   \n",
       "2   Wie viele gelbe Karten gabs es im Spiel Werder...   \n",
       "3   Wie viele gelbe Karten gabs es im Spiel Werder...   \n",
       "4   Wie viele gelbe Karten gabs es im Spiel Werder...   \n",
       "..                                                ...   \n",
       "91  Welcher Schiedsrichter hat die Begegnung Breme...   \n",
       "92  Welcher Schiedsrichter hat die Begegnung Breme...   \n",
       "93  Welcher Schiedsrichter hat die Begegnung Breme...   \n",
       "94  Welcher Schiedsrichter hat die Begegnung Breme...   \n",
       "95  Welcher Schiedsrichter hat die Begegnung Breme...   \n",
       "\n",
       "                                            inference   \n",
       "0   Die Notrufnummer der New Yorker Polizei ist 91...  \\\n",
       "1   2 Gelbe Karten gab es im Spiel Werder Bremen g...   \n",
       "2   2 Gelbe Karten gab es im Spiel Werder Bremen g...   \n",
       "3   2 Gelbe Karten gab es im Spiel Werder Bremen g...   \n",
       "4   2 Gelbe Karten gab es im Spiel Werder Bremen g...   \n",
       "..                                                ...   \n",
       "91                                              Brych   \n",
       "92                                              Brych   \n",
       "93  Ich kann diese Information nicht beantworten, ...   \n",
       "94                                       Felix Zwayer   \n",
       "95                                       Felix Zwayer   \n",
       "\n",
       "                                              context   \n",
       "0                                                  []  \\\n",
       "1   [(page_content='leverkusen hat die tabellenfuh...   \n",
       "2   [(page_content='leverkusen hat die tabellenfuh...   \n",
       "3   [(page_content='leverkusen hat die tabellenfuh...   \n",
       "4   [(page_content='leverkusen hat die tabellenfuh...   \n",
       "..                                                ...   \n",
       "91  [(page_content='leverkusen hat die tabellenfuh...   \n",
       "92  [(page_content='leverkusen hat die tabellenfuh...   \n",
       "93  [(page_content='leverkusen hat die tabellenfuh...   \n",
       "94  [(page_content='saison weiter ungeschlagenen l...   \n",
       "95  [(page_content='leverkusen hat die tabellenfuh...   \n",
       "\n",
       "                                  retrieve_methods  multi_query   \n",
       "0                                               []         True  \\\n",
       "1                                        [keyword]         True   \n",
       "2                                     [similarity]         True   \n",
       "3                                  [time_weighted]         True   \n",
       "4                                       [metadata]         True   \n",
       "..                                             ...          ...   \n",
       "91            [keyword, similarity, time_weighted]        False   \n",
       "92                 [keyword, similarity, metadata]        False   \n",
       "93              [keyword, time_weighted, metadata]        False   \n",
       "94           [similarity, time_weighted, metadata]        False   \n",
       "95  [keyword, similarity, time_weighted, metadata]        False   \n",
       "\n",
       "    remove_stopwords  inference_time  documents_in_scope   \n",
       "0              False       44.417751                  12  \\\n",
       "1              False       25.888465                  12   \n",
       "2              False       25.412266                  12   \n",
       "3              False       40.063634                  12   \n",
       "4              False       29.105342                  12   \n",
       "..               ...             ...                 ...   \n",
       "91             False       11.310008                  12   \n",
       "92             False        5.404342                  12   \n",
       "93             False       12.967905                  12   \n",
       "94             False       11.517587                  12   \n",
       "95             False       12.024930                  12   \n",
       "\n",
       "                                               kwargs  \n",
       "0   {'ground_truths': 'Es gab für Bremen drei gelb...  \n",
       "1   {'ground_truths': 'Es gab für Bremen drei gelb...  \n",
       "2   {'ground_truths': 'Es gab für Bremen drei gelb...  \n",
       "3   {'ground_truths': 'Es gab für Bremen drei gelb...  \n",
       "4   {'ground_truths': 'Es gab für Bremen drei gelb...  \n",
       "..                                                ...  \n",
       "91  {'ground_truths': 'Das Schiedsrichter-Gespann ...  \n",
       "92  {'ground_truths': 'Das Schiedsrichter-Gespann ...  \n",
       "93  {'ground_truths': 'Das Schiedsrichter-Gespann ...  \n",
       "94  {'ground_truths': 'Das Schiedsrichter-Gespann ...  \n",
       "95  {'ground_truths': 'Das Schiedsrichter-Gespann ...  \n",
       "\n",
       "[96 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d5ff9b9-8ac9-4da4-8359-6a3bccc12734",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "rag.save_test(\"multi_query_test_sk8x7B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
